{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85d28455-f407-408c-86dd-97585a6467bf",
   "metadata": {},
   "source": [
    "# RNN sentiment classification on IMDB csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77385321-daf5-4ba1-9558-12923a228394",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/thakkar/opt/anaconda3/envs/py39/lib/python3.9/site-packages/threadpoolctl.py:1214: RuntimeWarning: \n",
      "Found Intel OpenMP ('libiomp') and LLVM OpenMP ('libomp') loaded at\n",
      "the same time. Both libraries are known to be incompatible and this\n",
      "can cause random crashes or deadlocks on Linux when loaded in the\n",
      "same Python program.\n",
      "Using threadpoolctl may cause crashes or deadlocks. For more\n",
      "information and possible workarounds, please see\n",
      "    https://github.com/joblib/threadpoolctl/blob/master/multiple_openmp.md\n",
      "\n",
      "  warnings.warn(msg, RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "from torchtext import vocab as V\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "import time \n",
    "import random\n",
    "\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9c550b-d691-48bd-93f7-b43c608cd0f1",
   "metadata": {},
   "source": [
    "## General settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0040b1d1-3420-4b22-8aa4-57db19b08a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 123\n",
    "torch.manual_seed(random_seed)\n",
    "\n",
    "vocabulary_size = 2000\n",
    "learning_rate = 1e-4\n",
    "batch_size = 128\n",
    "num_epochs = 15\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "emnbedding_dim = 128\n",
    "hidden_dim = 256\n",
    "output_dim = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4861773c-dd8b-412e-9c62-a3a6bb07745c",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ccc8d8ed-9f57-4a9a-b3a9-083281139316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-06-18 17:39:51--  https://github.com/rasbt/python-machine-learning-book-2nd-edition/raw/master/code/ch08/movie_data.csv.gz\n",
      "Resolving github.com (github.com)... 140.82.114.4\n",
      "Connecting to github.com (github.com)|140.82.114.4|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://raw.githubusercontent.com/rasbt/python-machine-learning-book-2nd-edition/master/code/ch08/movie_data.csv.gz [following]\n",
      "--2024-06-18 17:39:52--  https://raw.githubusercontent.com/rasbt/python-machine-learning-book-2nd-edition/master/code/ch08/movie_data.csv.gz\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.109.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 26521894 (25M) [application/octet-stream]\n",
      "Saving to: ‘movie_data.csv.gz’\n",
      "\n",
      "movie_data.csv.gz   100%[===================>]  25.29M  49.8MB/s    in 0.5s    \n",
      "\n",
      "2024-06-18 17:39:53 (49.8 MB/s) - ‘movie_data.csv.gz’ saved [26521894/26521894]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://github.com/rasbt/python-machine-learning-book-2nd-edition/raw/master/code/ch08/movie_data.csv.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6eeb9664-6015-4552-bfcd-5a22f84e190e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gunzip -f movie_data.csv.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49a0cde2-27be-4ec7-a618-3c3510ac9f45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In 1974, the teenager Martha Moxley (Maggie Gr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OK... so... I really like Kris Kristofferson a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>***SPOILER*** Do not read this, if you think a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hi for all the people who have seen this wonde...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I recently bought the DVD, forgetting just how...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  In 1974, the teenager Martha Moxley (Maggie Gr...          1\n",
       "1  OK... so... I really like Kris Kristofferson a...          0\n",
       "2  ***SPOILER*** Do not read this, if you think a...          0\n",
       "3  hi for all the people who have seen this wonde...          1\n",
       "4  I recently bought the DVD, forgetting just how...          0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('movie_data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "028505de-6388-472d-be2b-edef585b26a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment\n",
       "1    25000\n",
       "0    25000\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae2f5f4-e9e3-44e7-b085-820d02e35a1c",
   "metadata": {},
   "source": [
    "## Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "203a6696-2145-42d3-9219-858443d802c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "tokenizer = data.utils.get_tokenizer('spacy')\n",
    "\n",
    "# counter = Counter()\n",
    "# for label, line in train_iter:\n",
    "#     counter.update(tokenizer(line))\n",
    "# vocab = V.vocab(counter, min_freq=10, specials=('<unk>', '<pad>'))\n",
    "\n",
    "def yield_tokens(data_iter):\n",
    "    for row in data_iter:\n",
    "        # yield tokenizer(row['review'])\n",
    "        yield tokenizer(row[1])\n",
    "\n",
    "\n",
    "def get_vocab(train_datapipe):\n",
    "    v = V.build_vocab_from_iterator(yield_tokens(train_datapipe),\n",
    "                                            specials=['<UNK>', '<PAD>'],\n",
    "                                            max_tokens=vocabulary_size+2)\n",
    "    v.set_default_index(v['<UNK>'])\n",
    "    return v\n",
    "\n",
    "# train_iter = IMDB(split='train')\n",
    "# vocab = get_vocab(df.iterrows())   # very slow\n",
    "vocab = get_vocab(df.itertuples())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "25376f1b-a75f-46f3-a069-0e832d1ed245",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2002"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8cac0893-084b-49c4-97c0-fdf7bb0cee79",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class IMDBDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, df, tokenizer):\n",
    "        super().__init__()\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        review = self.df.iloc[idx][\"review\"]\n",
    "        label = self.df.iloc[idx][\"sentiment\"]\n",
    "\n",
    "        tokens = self.tokenizer(review)\n",
    "        return label, tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "dd77dd15-6311-48dd-9f6a-5af84c213d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader \n",
    "from torch.nn.utils.rnn import pad_sequence \n",
    "\n",
    "# text_transform = lambda x: [vocab['<BOS>']] + [vocab[token] for token in tokenize(x)] + [vocab['<EOS>']]  \n",
    "# tokenizer already used above in the Dataset class\n",
    "text_transform = lambda x: [vocab['<BOS>']] + [vocab[token] for token in x] + [vocab['<EOS>']]\n",
    "# label_transform = lambda x: 1 if x == 'pos' else 0\n",
    "\n",
    "def collate_batch(batch): \n",
    "    label_list, text_list = [], [] \n",
    "  \n",
    "    for (_label, _text) in batch: \n",
    "        \n",
    "        # label_list.append(label_transform(_label)) \n",
    "        label_list.append(_label)\n",
    "        processed_text = torch.tensor(text_transform(_text)) \n",
    "        text_list.append(processed_text) \n",
    "    return torch.tensor(label_list), pad_sequence(text_list, padding_value=3.0) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a44a166f-1904-4a1c-b986-a7d0ee6b4b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## create train val test split\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=0)\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.1, random_state=0)\n",
    "\n",
    "train_dataset = IMDBDataset(train_df, tokenizer)\n",
    "val_dataset = IMDBDataset(val_df, tokenizer)\n",
    "test_dataset = IMDBDataset(test_df, tokenizer)\n",
    "\n",
    "# create the train and test dataloaders\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_batch)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_batch)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e583ed5c-80f2-4ca7-ba3c-450a9daf778a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128]) tensor([0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0,\n",
      "        1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1,\n",
      "        0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1,\n",
      "        1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1,\n",
      "        0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,\n",
      "        0, 1, 0, 0, 1, 0, 0, 0])\n",
      "torch.Size([1207, 128]) tensor([[  0,   0,   0,  ...,   0,   0,   0],\n",
      "        [  0, 407,  25,  ...,  56,  66, 899],\n",
      "        [ 97,   2,  78,  ..., 875,  21,  84],\n",
      "        ...,\n",
      "        [  3,   3,   3,  ...,   3,   3,   3],\n",
      "        [  3,   3,   3,  ...,   3,   3,   3],\n",
      "        [  3,   3,   3,  ...,   3,   3,   3]])\n"
     ]
    }
   ],
   "source": [
    "x,y = next(iter(train_loader))\n",
    "print(x.shape, x)\n",
    "print(y.shape, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08d69d9-0fd4-447d-bc12-55fdb5f910fc",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "36b2d972-ce73-4615-a34c-5c4f78475323",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim):\n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, text):\n",
    "\n",
    "        #[sentence len, batch size] => [sentence len, batch size, embedding size]\n",
    "        embedded = self.embedding(text)\n",
    "\n",
    "        #[sentence len, batch size, embedding size] => \n",
    "        #  output: [sentence len, batch size, hidden size]\n",
    "        #  hidden: [1, batch size, hidden size]\n",
    "        output, hidden = self.rnn(embedded)\n",
    "        \n",
    "        return self.fc(hidden.squeeze(0)).view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b1da6d22-430d-476b-944b-0cabe487484a",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = len(vocab)\n",
    "embedding_dim = 64\n",
    "hidden_dim = 128\n",
    "output_dim = 1\n",
    "\n",
    "torch.manual_seed(random_seed)\n",
    "model = RNN(input_dim, embedding_dim, hidden_dim, output_dim)\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3543daad-c7fd-411e-b627-a4cd3b9288ae",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b807ce10-c3d5-4efd-b35c-b680f09ce350",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_binary_accuracy(model, data_loader, device):\n",
    "    model.eval()\n",
    "    correct_pred, num_examples = 0,0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (label, text) in enumerate(data_loader):\n",
    "            logits = model(text)\n",
    "            predicted_labels = (torch.sigmoid(logits) > 0.5).long()\n",
    "            num_examples += label.size(0)\n",
    "            correct_pred += (predicted_labels == label.long()).sum()\n",
    "\n",
    "    return correct_pred.float()/num_examples * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "57822072-8f8f-4151-8863-15a492506842",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001/015 | Batch 000/282 | Cost: 0.6923\n",
      "Epoch: 001/015 | Batch 050/282 | Cost: 0.6935\n",
      "Epoch: 001/015 | Batch 100/282 | Cost: 0.6926\n",
      "Epoch: 001/015 | Batch 150/282 | Cost: 0.6957\n",
      "Epoch: 001/015 | Batch 200/282 | Cost: 0.7013\n",
      "Epoch: 001/015 | Batch 250/282 | Cost: 0.6939\n",
      "training accuracy: 49.83%\n",
      "valid accuracy: 49.08%\n",
      "Time elapsed: 4.87 min\n",
      "Epoch: 002/015 | Batch 000/282 | Cost: 0.6937\n",
      "Epoch: 002/015 | Batch 050/282 | Cost: 0.6934\n",
      "Epoch: 002/015 | Batch 100/282 | Cost: 0.6927\n",
      "Epoch: 002/015 | Batch 150/282 | Cost: 0.6933\n",
      "Epoch: 002/015 | Batch 200/282 | Cost: 0.6923\n",
      "Epoch: 002/015 | Batch 250/282 | Cost: 0.6933\n",
      "training accuracy: 49.87%\n",
      "valid accuracy: 48.95%\n",
      "Time elapsed: 10.58 min\n",
      "Epoch: 003/015 | Batch 000/282 | Cost: 0.6938\n",
      "Epoch: 003/015 | Batch 050/282 | Cost: 0.6926\n",
      "Epoch: 003/015 | Batch 100/282 | Cost: 0.6907\n",
      "Epoch: 003/015 | Batch 150/282 | Cost: 0.6924\n",
      "Epoch: 003/015 | Batch 200/282 | Cost: 0.6934\n",
      "Epoch: 003/015 | Batch 250/282 | Cost: 0.6998\n",
      "training accuracy: 50.27%\n",
      "valid accuracy: 51.15%\n",
      "Time elapsed: 15.99 min\n",
      "Epoch: 004/015 | Batch 000/282 | Cost: 0.6931\n",
      "Epoch: 004/015 | Batch 050/282 | Cost: 0.6924\n",
      "Epoch: 004/015 | Batch 100/282 | Cost: 0.6925\n",
      "Epoch: 004/015 | Batch 150/282 | Cost: 0.6930\n",
      "Epoch: 004/015 | Batch 200/282 | Cost: 0.6867\n",
      "Epoch: 004/015 | Batch 250/282 | Cost: 0.6929\n",
      "training accuracy: 49.85%\n",
      "valid accuracy: 49.05%\n",
      "Time elapsed: 21.07 min\n",
      "Epoch: 005/015 | Batch 000/282 | Cost: 0.6919\n",
      "Epoch: 005/015 | Batch 050/282 | Cost: 0.6923\n",
      "Epoch: 005/015 | Batch 100/282 | Cost: 0.6952\n",
      "Epoch: 005/015 | Batch 150/282 | Cost: 0.6976\n",
      "Epoch: 005/015 | Batch 200/282 | Cost: 0.6916\n",
      "Epoch: 005/015 | Batch 250/282 | Cost: 0.6924\n",
      "training accuracy: 49.86%\n",
      "valid accuracy: 48.95%\n",
      "Time elapsed: 26.77 min\n",
      "Epoch: 006/015 | Batch 000/282 | Cost: 0.6901\n",
      "Epoch: 006/015 | Batch 050/282 | Cost: 0.6932\n",
      "Epoch: 006/015 | Batch 100/282 | Cost: 0.6909\n",
      "Epoch: 006/015 | Batch 150/282 | Cost: 0.6942\n",
      "Epoch: 006/015 | Batch 200/282 | Cost: 0.6942\n",
      "Epoch: 006/015 | Batch 250/282 | Cost: 0.6943\n",
      "training accuracy: 50.31%\n",
      "valid accuracy: 51.08%\n",
      "Time elapsed: 32.22 min\n",
      "Epoch: 007/015 | Batch 000/282 | Cost: 0.6921\n",
      "Epoch: 007/015 | Batch 050/282 | Cost: 0.6929\n",
      "Epoch: 007/015 | Batch 100/282 | Cost: 0.6922\n",
      "Epoch: 007/015 | Batch 150/282 | Cost: 0.6883\n",
      "Epoch: 007/015 | Batch 200/282 | Cost: 0.6944\n",
      "Epoch: 007/015 | Batch 250/282 | Cost: 0.6882\n",
      "training accuracy: 49.85%\n",
      "valid accuracy: 49.08%\n",
      "Time elapsed: 37.66 min\n",
      "Epoch: 008/015 | Batch 000/282 | Cost: 0.6901\n",
      "Epoch: 008/015 | Batch 050/282 | Cost: 0.6916\n",
      "Epoch: 008/015 | Batch 100/282 | Cost: 0.6906\n",
      "Epoch: 008/015 | Batch 150/282 | Cost: 0.6921\n",
      "Epoch: 008/015 | Batch 200/282 | Cost: 0.6913\n",
      "Epoch: 008/015 | Batch 250/282 | Cost: 0.6932\n",
      "training accuracy: 49.89%\n",
      "valid accuracy: 48.90%\n",
      "Time elapsed: 43.04 min\n",
      "Epoch: 009/015 | Batch 000/282 | Cost: 0.6917\n",
      "Epoch: 009/015 | Batch 050/282 | Cost: 0.6927\n",
      "Epoch: 009/015 | Batch 100/282 | Cost: 0.6912\n",
      "Epoch: 009/015 | Batch 150/282 | Cost: 0.6919\n",
      "Epoch: 009/015 | Batch 200/282 | Cost: 0.6947\n",
      "Epoch: 009/015 | Batch 250/282 | Cost: 0.6916\n",
      "training accuracy: 49.84%\n",
      "valid accuracy: 48.75%\n",
      "Time elapsed: 48.54 min\n",
      "Epoch: 010/015 | Batch 000/282 | Cost: 0.6917\n",
      "Epoch: 010/015 | Batch 050/282 | Cost: 0.6901\n",
      "Epoch: 010/015 | Batch 100/282 | Cost: 0.6949\n",
      "Epoch: 010/015 | Batch 150/282 | Cost: 0.6913\n",
      "Epoch: 010/015 | Batch 200/282 | Cost: 0.6929\n",
      "Epoch: 010/015 | Batch 250/282 | Cost: 0.6935\n",
      "training accuracy: 49.92%\n",
      "valid accuracy: 49.08%\n",
      "Time elapsed: 54.18 min\n",
      "Epoch: 011/015 | Batch 000/282 | Cost: 0.6929\n",
      "Epoch: 011/015 | Batch 050/282 | Cost: 0.6935\n",
      "Epoch: 011/015 | Batch 100/282 | Cost: 0.6917\n",
      "Epoch: 011/015 | Batch 150/282 | Cost: 0.6929\n",
      "Epoch: 011/015 | Batch 200/282 | Cost: 0.6972\n",
      "Epoch: 011/015 | Batch 250/282 | Cost: 0.6929\n",
      "training accuracy: 50.33%\n",
      "valid accuracy: 51.17%\n",
      "Time elapsed: 59.42 min\n",
      "Epoch: 012/015 | Batch 000/282 | Cost: 0.6911\n",
      "Epoch: 012/015 | Batch 050/282 | Cost: 0.6932\n",
      "Epoch: 012/015 | Batch 100/282 | Cost: 0.6901\n",
      "Epoch: 012/015 | Batch 150/282 | Cost: 0.6903\n",
      "Epoch: 012/015 | Batch 200/282 | Cost: 0.6955\n",
      "Epoch: 012/015 | Batch 250/282 | Cost: 0.6953\n",
      "training accuracy: 50.34%\n",
      "valid accuracy: 51.08%\n",
      "Time elapsed: 64.83 min\n",
      "Epoch: 013/015 | Batch 000/282 | Cost: 0.6917\n",
      "Epoch: 013/015 | Batch 050/282 | Cost: 0.6906\n",
      "Epoch: 013/015 | Batch 100/282 | Cost: 0.6905\n",
      "Epoch: 013/015 | Batch 150/282 | Cost: 0.6949\n",
      "Epoch: 013/015 | Batch 200/282 | Cost: 0.6947\n",
      "Epoch: 013/015 | Batch 250/282 | Cost: 0.6941\n",
      "training accuracy: 49.93%\n",
      "valid accuracy: 48.88%\n",
      "Time elapsed: 70.10 min\n",
      "Epoch: 014/015 | Batch 000/282 | Cost: 0.6951\n",
      "Epoch: 014/015 | Batch 050/282 | Cost: 0.6912\n",
      "Epoch: 014/015 | Batch 100/282 | Cost: 0.6919\n",
      "Epoch: 014/015 | Batch 150/282 | Cost: 0.6917\n",
      "Epoch: 014/015 | Batch 200/282 | Cost: 0.6874\n",
      "Epoch: 014/015 | Batch 250/282 | Cost: 0.6974\n",
      "training accuracy: 50.30%\n",
      "valid accuracy: 51.12%\n",
      "Time elapsed: 75.95 min\n",
      "Epoch: 015/015 | Batch 000/282 | Cost: 0.6917\n",
      "Epoch: 015/015 | Batch 050/282 | Cost: 0.6924\n",
      "Epoch: 015/015 | Batch 100/282 | Cost: 0.6914\n",
      "Epoch: 015/015 | Batch 150/282 | Cost: 0.6967\n",
      "Epoch: 015/015 | Batch 200/282 | Cost: 0.6931\n",
      "Epoch: 015/015 | Batch 250/282 | Cost: 0.6924\n",
      "training accuracy: 49.93%\n",
      "valid accuracy: 49.00%\n",
      "Time elapsed: 82.10 min\n",
      "Total Training Time: 82.10 min\n",
      "Test accuracy: 51.00%\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch_idx, (label, text) in enumerate(train_loader):\n",
    "        \n",
    "        ### FORWARD AND BACK PROP\n",
    "        logits = model(text)\n",
    "        cost = F.binary_cross_entropy_with_logits(logits, label.float())\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        cost.backward()\n",
    "        \n",
    "        ### UPDATE MODEL PARAMETERS\n",
    "        optimizer.step()\n",
    "        \n",
    "        ### LOGGING\n",
    "        if not batch_idx % 50:\n",
    "            print (f'Epoch: {epoch+1:03d}/{num_epochs:03d} | '\n",
    "                   f'Batch {batch_idx:03d}/{len(list(train_loader)):03d} | '\n",
    "                   f'Cost: {cost:.4f}')\n",
    "\n",
    "    with torch.set_grad_enabled(False):\n",
    "        print(f'training accuracy: '\n",
    "              f'{compute_binary_accuracy(model, train_loader, device):.2f}%'\n",
    "              f'\\nvalid accuracy: '\n",
    "              f'{compute_binary_accuracy(model, val_loader, device):.2f}%')\n",
    "        \n",
    "    print(f'Time elapsed: {(time.time() - start_time)/60:.2f} min')\n",
    "    \n",
    "print(f'Total Training Time: {(time.time() - start_time)/60:.2f} min')\n",
    "print(f'Test accuracy: {compute_binary_accuracy(model, test_loader, device):.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "8a8171ea-b1d3-4cb9-b149-67562f77d2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Predict sentiment from sentence\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def predict_sentiment(model, sentence):\n",
    "    # based on:\n",
    "    # https://github.com/bentrevett/pytorch-sentiment-analysis/blob/\n",
    "    # master/2%20-%20Upgraded%20Sentiment%20Analysis.ipynb\n",
    "    model.eval()\n",
    "    tokenized = [tok.text for tok in nlp.tokenizer(sentence)]\n",
    "    indexed = [vocab.get_stoi()[t] for t in tokenized]\n",
    "    tensor = torch.LongTensor(indexed).to(device)\n",
    "    tensor = tensor.unsqueeze(1)\n",
    "    prediction = torch.sigmoid(model(tensor))\n",
    "    return prediction.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "69a21bfa-262c-4715-910b-01d375b2e98b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability positive:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.520069420337677"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Probability positive:')\n",
    "predict_sentiment(model, \"I really love this movie. This movie is so great!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882bd545-7fb2-4b58-aad3-2598d68dc3fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
