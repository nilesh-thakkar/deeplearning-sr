{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85d28455-f407-408c-86dd-97585a6467bf",
   "metadata": {},
   "source": [
    "# RNN sentiment classification on IMDB csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77385321-daf5-4ba1-9558-12923a228394",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/thakkar/opt/anaconda3/envs/py39/lib/python3.9/site-packages/threadpoolctl.py:1214: RuntimeWarning: \n",
      "Found Intel OpenMP ('libiomp') and LLVM OpenMP ('libomp') loaded at\n",
      "the same time. Both libraries are known to be incompatible and this\n",
      "can cause random crashes or deadlocks on Linux when loaded in the\n",
      "same Python program.\n",
      "Using threadpoolctl may cause crashes or deadlocks. For more\n",
      "information and possible workarounds, please see\n",
      "    https://github.com/joblib/threadpoolctl/blob/master/multiple_openmp.md\n",
      "\n",
      "  warnings.warn(msg, RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "from torchtext import vocab as V\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "import time \n",
    "import random\n",
    "\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9c550b-d691-48bd-93f7-b43c608cd0f1",
   "metadata": {},
   "source": [
    "## General settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0040b1d1-3420-4b22-8aa4-57db19b08a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 123\n",
    "torch.manual_seed(random_seed)\n",
    "\n",
    "vocabulary_size = 20000\n",
    "learning_rate = 1e-4\n",
    "batch_size = 128\n",
    "num_epochs = 15\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "embedding_dim = 128\n",
    "hidden_dim = 256\n",
    "output_dim = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4861773c-dd8b-412e-9c62-a3a6bb07745c",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ccc8d8ed-9f57-4a9a-b3a9-083281139316",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://github.com/rasbt/python-machine-learning-book-2nd-edition/raw/master/code/ch08/movie_data.csv.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6eeb9664-6015-4552-bfcd-5a22f84e190e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !gunzip -f movie_data.csv.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49a0cde2-27be-4ec7-a618-3c3510ac9f45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In 1974, the teenager Martha Moxley (Maggie Gr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OK... so... I really like Kris Kristofferson a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>***SPOILER*** Do not read this, if you think a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hi for all the people who have seen this wonde...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I recently bought the DVD, forgetting just how...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  In 1974, the teenager Martha Moxley (Maggie Gr...          1\n",
       "1  OK... so... I really like Kris Kristofferson a...          0\n",
       "2  ***SPOILER*** Do not read this, if you think a...          0\n",
       "3  hi for all the people who have seen this wonde...          1\n",
       "4  I recently bought the DVD, forgetting just how...          0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('movie_data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "028505de-6388-472d-be2b-edef585b26a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment\n",
       "1    25000\n",
       "0    25000\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae2f5f4-e9e3-44e7-b085-820d02e35a1c",
   "metadata": {},
   "source": [
    "## Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "203a6696-2145-42d3-9219-858443d802c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/thakkar/opt/anaconda3/envs/py39/lib/python3.9/site-packages/torchtext/data/utils.py:105: UserWarning: Spacy model \"en\" could not be loaded, trying \"en_core_web_sm\" instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "tokenizer = data.utils.get_tokenizer('spacy')\n",
    "\n",
    "# counter = Counter()\n",
    "# for label, line in train_iter:\n",
    "#     counter.update(tokenizer(line))\n",
    "# vocab = V.vocab(counter, min_freq=10, specials=('<unk>', '<pad>'))\n",
    "\n",
    "def yield_tokens(data_iter):\n",
    "    for row in data_iter:\n",
    "        # yield tokenizer(row['review'])\n",
    "        yield tokenizer(row[1])\n",
    "\n",
    "\n",
    "def get_vocab(train_datapipe):\n",
    "    v = V.build_vocab_from_iterator(yield_tokens(train_datapipe),\n",
    "                                            specials=['<UNK>', '<PAD>'],\n",
    "                                            max_tokens=vocabulary_size+2)\n",
    "    v.set_default_index(v['<UNK>'])\n",
    "    return v\n",
    "\n",
    "# train_iter = IMDB(split='train')\n",
    "# vocab = get_vocab(df.iterrows())   # very slow\n",
    "vocab = get_vocab(df.itertuples())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25376f1b-a75f-46f3-a069-0e832d1ed245",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20002"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8cac0893-084b-49c4-97c0-fdf7bb0cee79",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class IMDBDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, df, tokenizer):\n",
    "        super().__init__()\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        review = self.df.iloc[idx][\"review\"]\n",
    "        label = self.df.iloc[idx][\"sentiment\"]\n",
    "\n",
    "        tokens = self.tokenizer(review)\n",
    "        return label, tokens, len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd77dd15-6311-48dd-9f6a-5af84c213d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader \n",
    "from torch.nn.utils.rnn import pad_sequence \n",
    "\n",
    "# text_transform = lambda x: [vocab['<BOS>']] + [vocab[token] for token in tokenize(x)] + [vocab['<EOS>']]  \n",
    "# tokenizer already used above in the Dataset class\n",
    "text_transform = lambda x: [vocab['<BOS>']] + [vocab[token] for token in x] + [vocab['<EOS>']]\n",
    "# label_transform = lambda x: 1 if x == 'pos' else 0\n",
    "\n",
    "def collate_batch(batch): \n",
    "    # Sort the batch in the descending order req for packed padded seq\n",
    "    # sorted_batch = sorted(batch, key=lambda x: x[2], reverse=True)\n",
    "    \n",
    "    label_list, text_list, length_list = [], [] , []\n",
    "  \n",
    "    for (_label, _text, _length) in batch: \n",
    "        \n",
    "        # label_list.append(label_transform(_label)) \n",
    "        label_list.append(_label)\n",
    "        processed_text = torch.tensor(text_transform(_text)) \n",
    "        text_list.append(processed_text) \n",
    "        length_list.append(_length)\n",
    "    return torch.tensor(label_list), pad_sequence(text_list, padding_value=3.0), torch.LongTensor(length_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a44a166f-1904-4a1c-b986-a7d0ee6b4b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## create train val test split\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=0)\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.1, random_state=0)\n",
    "\n",
    "train_dataset = IMDBDataset(train_df, tokenizer)\n",
    "val_dataset = IMDBDataset(val_df, tokenizer)\n",
    "test_dataset = IMDBDataset(test_df, tokenizer)\n",
    "\n",
    "# create the train and test dataloaders\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_batch)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_batch)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e583ed5c-80f2-4ca7-ba3c-450a9daf778a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128]) tensor([0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0,\n",
      "        1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0,\n",
      "        0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1,\n",
      "        1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0,\n",
      "        1, 1, 1, 0, 1, 0, 1, 0])\n",
      "torch.Size([1188, 128]) tensor([[   0,    0,    0,  ...,    0,    0,    0],\n",
      "        [6563, 1063,  320,  ...,  348,  925,   25],\n",
      "        [6001,   15,   11,  ...,   91,  469,   75],\n",
      "        ...,\n",
      "        [   3,    3,    3,  ...,    3,    3,    3],\n",
      "        [   3,    3,    3,  ...,    3,    3,    3],\n",
      "        [   3,    3,    3,  ...,    3,    3,    3]])\n",
      "torch.Size([128]) tensor([  61,  375,  272,  243,  584,  207,  164,   75,  376,  116,  311,  179,\n",
      "         531,  341,  277,   75,  150,  395,   46,   68,  674,  234,  267,  159,\n",
      "         371,  168,   56,  204,  177,  160,  256,  168,  294,  152,  109,  212,\n",
      "         121,  197,  604,  157, 1019,  153,  122,  143,  181,  212,  162,  175,\n",
      "         255,  253,   64,  226,  143,  207,  528,  142,  184,  136,  328,  255,\n",
      "         179,  230,  279,  222,  432,  919,  176,  304,  206,  239,  101,  366,\n",
      "         419,  756,  198,  357,  392,  187,  212,  154,  133,  290,  193,  320,\n",
      "         418,  221,   57,  202,  445,  379,  479,  181,  210,  255,  169,  171,\n",
      "          58,  235,  317,  387,  236,  523,  148,  667, 1056,  142,  221,  259,\n",
      "         302,   83,  691,  135,  129,  145,  175,  206,  288,  657,   54, 1186,\n",
      "         205,  406,  253,  476,  149,  385,  143,   56])\n"
     ]
    }
   ],
   "source": [
    "x,y,z = next(iter(train_loader))\n",
    "print(x.shape, x)\n",
    "print(y.shape, y)\n",
    "print(z.shape, z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08d69d9-0fd4-447d-bc12-55fdb5f910fc",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "36b2d972-ce73-4615-a34c-5c4f78475323",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim):\n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
    "        # self.rnn = nn.RNN(embedding_dim, hidden_dim)\n",
    "        self.rnn = nn.LSTM(embedding_dim, hidden_dim)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, text, text_length):\n",
    "\n",
    "        #[sentence len, batch size] => [sentence len, batch size, embedding size]\n",
    "        embedded = self.embedding(text)\n",
    "\n",
    "        # packed = torch.nn.utils.rnn.pack_padded_sequence(embedded, text_length)\n",
    "        \n",
    "        #[sentence len, batch size, embedding size] => \n",
    "        #  output: [sentence len, batch size, hidden size]\n",
    "        #  hidden: [1, batch size, hidden size]\n",
    "        # output, hidden = self.rnn(embedded)\n",
    "        output, (hidden, cell) = self.rnn(embedded)\n",
    "        # packed_output, (hidden, cell) = self.rnn(packed)\n",
    "        \n",
    "        return self.fc(hidden.squeeze(0)).view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b1da6d22-430d-476b-944b-0cabe487484a",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = len(vocab)\n",
    "\n",
    "torch.manual_seed(random_seed)\n",
    "model = RNN(input_dim, embedding_dim, hidden_dim, output_dim)\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3543daad-c7fd-411e-b627-a4cd3b9288ae",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b807ce10-c3d5-4efd-b35c-b680f09ce350",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_binary_accuracy(model, data_loader, device):\n",
    "    model.eval()\n",
    "    correct_pred, num_examples = 0,0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (label, text, text_length) in enumerate(data_loader):\n",
    "            logits = model(text, text_length)\n",
    "            predicted_labels = (torch.sigmoid(logits) > 0.5).long()\n",
    "            num_examples += label.size(0)\n",
    "            correct_pred += (predicted_labels == label.long()).sum()\n",
    "\n",
    "    return correct_pred.float()/num_examples * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "57822072-8f8f-4151-8863-15a492506842",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001/015 | Batch 000/282 | Cost: 0.6926\n",
      "Epoch: 001/015 | Batch 050/282 | Cost: 0.6932\n",
      "Epoch: 001/015 | Batch 100/282 | Cost: 0.6946\n",
      "Epoch: 001/015 | Batch 150/282 | Cost: 0.6970\n",
      "Epoch: 001/015 | Batch 200/282 | Cost: 0.6933\n",
      "Epoch: 001/015 | Batch 250/282 | Cost: 0.6937\n",
      "training accuracy: 50.18%\n",
      "valid accuracy: 51.08%\n",
      "Time elapsed: 35.11 min\n",
      "Epoch: 002/015 | Batch 000/282 | Cost: 0.6923\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m cost \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mbinary_cross_entropy_with_logits(logits, label\u001b[38;5;241m.\u001b[39mfloat())\n\u001b[1;32m     10\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 12\u001b[0m \u001b[43mcost\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m### UPDATE MODEL PARAMETERS\u001b[39;00m\n\u001b[1;32m     15\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/py39/lib/python3.9/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/py39/lib/python3.9/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch_idx, (label, text, text_length) in enumerate(train_loader):\n",
    "        \n",
    "        ### FORWARD AND BACK PROP\n",
    "        logits = model(text, text_length)\n",
    "        cost = F.binary_cross_entropy_with_logits(logits, label.float())\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        cost.backward()\n",
    "        \n",
    "        ### UPDATE MODEL PARAMETERS\n",
    "        optimizer.step()\n",
    "        \n",
    "        ### LOGGING\n",
    "        if not batch_idx % 50:\n",
    "            print (f'Epoch: {epoch+1:03d}/{num_epochs:03d} | '\n",
    "                   f'Batch {batch_idx:03d}/{len(list(train_loader)):03d} | '\n",
    "                   f'Cost: {cost:.4f}')\n",
    "\n",
    "    with torch.set_grad_enabled(False):\n",
    "        print(f'training accuracy: '\n",
    "              f'{compute_binary_accuracy(model, train_loader, device):.2f}%'\n",
    "              f'\\nvalid accuracy: '\n",
    "              f'{compute_binary_accuracy(model, val_loader, device):.2f}%')\n",
    "        \n",
    "    print(f'Time elapsed: {(time.time() - start_time)/60:.2f} min')\n",
    "    \n",
    "print(f'Total Training Time: {(time.time() - start_time)/60:.2f} min')\n",
    "print(f'Test accuracy: {compute_binary_accuracy(model, test_loader, device):.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8171ea-b1d3-4cb9-b149-67562f77d2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Predict sentiment from sentence\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def predict_sentiment(model, sentence):\n",
    "    # based on:\n",
    "    # https://github.com/bentrevett/pytorch-sentiment-analysis/blob/\n",
    "    # master/2%20-%20Upgraded%20Sentiment%20Analysis.ipynb\n",
    "    model.eval()\n",
    "    tokenized = [tok.text for tok in nlp.tokenizer(sentence)]\n",
    "    indexed = [TEXT.vocab.stoi[t] for t in tokenized]\n",
    "    length = [len(indexed)]\n",
    "    tensor = torch.LongTensor(indexed).to(DEVICE)\n",
    "    tensor = tensor.unsqueeze(1)\n",
    "    length_tensor = torch.LongTensor(length)\n",
    "    prediction = torch.sigmoid(model(tensor, length_tensor))\n",
    "    return prediction.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a21bfa-262c-4715-910b-01d375b2e98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Probability positive:')\n",
    "predict_sentiment(model, \"I really love this movie. This movie is so great!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882bd545-7fb2-4b58-aad3-2598d68dc3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Probability negative:')\n",
    "predict_sentiment(model, \"I really hate this movie. It is really bad and sucks!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
