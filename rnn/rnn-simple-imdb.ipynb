{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "adeac7b5-ce42-4d82-98c2-e38aa661f3fa",
   "metadata": {},
   "source": [
    "# RNN for sentiment classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72c301a0-0941-4e8b-a4a9-007c3fdb5336",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/thakkar/opt/anaconda3/envs/py39/lib/python3.9/site-packages/threadpoolctl.py:1214: RuntimeWarning: \n",
      "Found Intel OpenMP ('libiomp') and LLVM OpenMP ('libomp') loaded at\n",
      "the same time. Both libraries are known to be incompatible and this\n",
      "can cause random crashes or deadlocks on Linux when loaded in the\n",
      "same Python program.\n",
      "Using threadpoolctl may cause crashes or deadlocks. For more\n",
      "information and possible workarounds, please see\n",
      "    https://github.com/joblib/threadpoolctl/blob/master/multiple_openmp.md\n",
      "\n",
      "  warnings.warn(msg, RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "from torchtext import vocab as V\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "import time \n",
    "import random\n",
    "\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775e4567-76c6-4b8b-b092-7d05567ca177",
   "metadata": {},
   "source": [
    "## General Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f567321-4069-49f1-a90b-2f4a47397a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 123\n",
    "torch.manual_seed(random_seed)\n",
    "\n",
    "vocabulary_size = 2000\n",
    "learning_rate = 1e-4\n",
    "batch_size = 128\n",
    "num_epochs = 15\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "emnbedding_dim = 128\n",
    "hidden_dim = 256\n",
    "output_dim = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de88e9bd-0691-40d6-bfa4-12d70de3be5a",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59377591-c092-4a3c-95d3-78cb0eb63062",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Train: 12500\n",
      "Num Test: 25000\n"
     ]
    }
   ],
   "source": [
    "## load IMDB dataset\n",
    "\n",
    "# text = data.Field(tokenize = 'spacy')\n",
    "# label = data.LabelField(dtype = torch.float)\n",
    "# train_data, test_data = datasets.IMDB.splits(text, label)\n",
    "# train_data, valid_data = train_data.split(random_state=random_seed.seed(random_seed),\n",
    "#                                           split_ratio=0.8)\n",
    "\n",
    "train_iter, test_iter = datasets.IMDB(split=('train', 'test'))\n",
    "# train_iter, valid_iter = train_iter.random_split(total_length=25000, weights={\"train\": 0.8, \"valid\": 0.2}, seed=0)\n",
    "\n",
    "train_data = list(train_iter)\n",
    "# valid_data = list(valid_iter)\n",
    "test_data = list(test_iter)\n",
    "\n",
    "print(f'Num Train: {len(train_data)}')\n",
    "# print(f'Num Valid: {len(valid_data)}')\n",
    "print(f'Num Test: {len(test_data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5845071e-9c8f-417f-8de3-bee43599b437",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/thakkar/opt/anaconda3/envs/py39/lib/python3.9/site-packages/torchtext/data/utils.py:105: UserWarning: Spacy model \"en\" could not be loaded, trying \"en_core_web_sm\" instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = data.utils.get_tokenizer('spacy')\n",
    "\n",
    "# counter = Counter()\n",
    "# for label, line in train_iter:\n",
    "#     counter.update(tokenizer(line))\n",
    "# vocab = V.vocab(counter, min_freq=10, specials=('<unk>', '<pad>'))\n",
    "\n",
    "def yield_tokens(data_iter):\n",
    "    for _, text in data_iter:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "\n",
    "def get_vocab(train_datapipe):\n",
    "    v = V.build_vocab_from_iterator(yield_tokens(train_datapipe),\n",
    "                                            specials=['<UNK>', '<PAD>'],\n",
    "                                            max_tokens=vocabulary_size+2)\n",
    "    v.set_default_index(v['<UNK>'])\n",
    "    return v\n",
    "\n",
    "# train_iter = IMDB(split='train')\n",
    "vocab = get_vocab(train_iter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef194c51-a0c8-41aa-b75c-965373e4fba6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2002"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "059563b2-ae68-401c-bdb2-dd56830284b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader \n",
    "from torch.nn.utils.rnn import pad_sequence \n",
    "\n",
    "text_transform = lambda x: [vocab['<BOS>']] + [vocab[token] for token in tokenizer(x)] + [vocab['<EOS>']]\n",
    "# label_transform = lambda x: 1 if x == 'pos' else 0\n",
    "\n",
    "def collate_batch(batch): \n",
    "    label_list, text_list = [], [] \n",
    "  \n",
    "    for (_label, _text) in batch: \n",
    "        # label_list.append(label_transform(_label)) \n",
    "        label_list.append(_label) \n",
    "        processed_text = torch.tensor(text_transform(_text)) \n",
    "        text_list.append(processed_text) \n",
    "    return torch.tensor(label_list), pad_sequence(text_list, padding_value=3.0) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7d727253-6c5d-4735-b6e3-29993282024b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1]), tensor([[   0,    0,    0,  ...,    0,    0,    0],\n",
      "        [1672,   15,   10,  ...,   56,    0,  159],\n",
      "        [   4,    0,  210,  ...,  215,   80,    0],\n",
      "        ...,\n",
      "        [   3,    3,    3,  ...,    3,    3,    3],\n",
      "        [   3,    3,    3,  ...,    3,    3,    3],\n",
      "        [   3,    3,    3,  ...,    3,    3,    3]]))\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Sampler\n",
    "\n",
    "class BatchSamplerSimilarLength(Sampler):\n",
    "  def __init__(self, dataset, batch_size, indices=None, shuffle=True):\n",
    "    self.batch_size = batch_size\n",
    "    self.shuffle = shuffle\n",
    "    # get the indices and length\n",
    "    self.indices = [(i, len(tokenizer(s[1]))) for i, s in enumerate(dataset)]\n",
    "    # if indices are passed, then use only the ones passed (for ddp)\n",
    "    if indices is not None:\n",
    "       self.indices = torch.tensor(self.indices)[indices].tolist()\n",
    "\n",
    "  def __iter__(self):\n",
    "    if self.shuffle:\n",
    "       random.shuffle(self.indices)\n",
    "\n",
    "    pooled_indices = []\n",
    "    # create pool of indices with similar lengths\n",
    "    for i in range(0, len(self.indices), self.batch_size * 100):\n",
    "      pooled_indices.extend(sorted(self.indices[i:i + self.batch_size * 100], key=lambda x: x[1]))\n",
    "    self.pooled_indices = [x[0] for x in pooled_indices]\n",
    "\n",
    "    # yield indices for current batch\n",
    "    batches = [self.pooled_indices[i:i + self.batch_size] for i in\n",
    "               range(0, len(self.pooled_indices), self.batch_size)]\n",
    "\n",
    "    if self.shuffle:\n",
    "        random.shuffle(batches)\n",
    "    for batch in batches:\n",
    "        yield batch\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.pooled_indices) // self.batch_size\n",
    "\n",
    "# def batch_sampler():\n",
    "#     indices = [(i, len(tokenizer(s[1]))) for i, s in enumerate(train_list)]\n",
    "#     random.shuffle(indices)\n",
    "#     pooled_indices  = []\n",
    "#     # create pool of indices with similar lengths \n",
    "#     for i in range(0, len(indices), batch_size * 100):\n",
    "#         pooled_indices.extend(sorted(indices[i:i + batch_size * 100], key=lambda x: x[1]))\n",
    "\n",
    "#     pooled_indices = [x[0] for x in pooled_indices]\n",
    "\n",
    "#     # yield indices for current batch\n",
    "#     for i in range(0, len(pooled_indices), batch_size):\n",
    "#         yield pooled_indices[i:i + batch_size]\n",
    "\n",
    "# sample_dataloader = DataLoader(list(train_iter), \n",
    "#                                batch_sampler=BatchSamplerSimilarLength(dataset=list(train_iter), batch_size=batch_size),\n",
    "#                                collate_fn=collate_batch)\n",
    "sample_dataloader = DataLoader(list(train_iter), \n",
    "                               batch_size=batch_size,\n",
    "                               collate_fn=collate_batch, \n",
    "                                shuffle=True)\n",
    "\n",
    "\n",
    "print(next(iter(sample_dataloader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e09f06a7-e4ab-4f1e-b05f-fa9998bea80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(list(train_iter),\n",
    "                          batch_sampler=BatchSamplerSimilarLength(dataset=list(train_iter), batch_size=batch_size),\n",
    "                          collate_fn=collate_batch, shuffle=True)\n",
    "# valid_loader = DataLoader(list(valid_iter),\n",
    "#                           batch_sampler=BatchSamplerSimilarLength(dataset=list(valid_iter), batch_size=batch_size),\n",
    "#                           collate_fn=collate_batch)\n",
    "test_loader = DataLoader(list(test_iter),\n",
    "                          batch_sampler=BatchSamplerSimilarLength(dataset=list(test_iter), batch_size=batch_size),\n",
    "                          collate_fn=collate_batch, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcfab351-008b-46e0-80dd-039388a4777c",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2300d0ee-79c3-4646-82ed-0e2536c637e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim):\n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, text):\n",
    "\n",
    "        #[sentence len, batch size] => [sentence len, batch size, embedding size]\n",
    "        embedded = self.embedding(text)\n",
    "\n",
    "        #[sentence len, batch size, embedding size] => \n",
    "        #  output: [sentence len, batch size, hidden size]\n",
    "        #  hidden: [1, batch size, hidden size]\n",
    "        output, hidden = self.rnn(embedded)\n",
    "        \n",
    "        return self.fc(hidden.squeeze(0)).view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "68a102b6-d33d-4381-bafc-3ee37c8342cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = len(vocab)\n",
    "embedding_dim = 64\n",
    "hidden_dim = 128\n",
    "output_dim = 1\n",
    "\n",
    "torch.manual_seed(random_seed)\n",
    "model = RNN(input_dim, embedding_dim, hidden_dim, output_dim)\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94ed3ba-7a1c-4d02-8748-bac62486cbf2",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a91631c2-c322-47a3-8b16-10a69a76c5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_binary_accuracy(model, data_loader, device):\n",
    "    model.eval()\n",
    "    correct_pred, num_examples = 0,0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (label, text) in enumerate(data_loader):\n",
    "            logits = model(text)\n",
    "            predicted_labels = (torch.sigmoid(logits) > 0.5).long()\n",
    "            num_examples += label.size(0)\n",
    "            correct_pred += (predicted_labels == label.long()).sum()\n",
    "\n",
    "    return correct_pred.float()/num_examples * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "12984cdb-a121-4266-9fb6-57565728f630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001/015 | Batch 000/098 | Cost: 0.8433\n",
      "Epoch: 001/015 | Batch 050/098 | Cost: 0.2869\n",
      "training accuracy: 100.00%\n",
      "Time elapsed: 1.06 min\n",
      "Epoch: 002/015 | Batch 000/098 | Cost: 0.0662\n",
      "Epoch: 002/015 | Batch 050/098 | Cost: 0.0349\n",
      "training accuracy: 100.00%\n",
      "Time elapsed: 2.20 min\n",
      "Epoch: 003/015 | Batch 000/098 | Cost: 0.0155\n",
      "Epoch: 003/015 | Batch 050/098 | Cost: 0.0116\n",
      "training accuracy: 100.00%\n",
      "Time elapsed: 3.24 min\n",
      "Epoch: 004/015 | Batch 000/098 | Cost: 0.0088\n",
      "Epoch: 004/015 | Batch 050/098 | Cost: 0.0080\n",
      "training accuracy: 100.00%\n",
      "Time elapsed: 4.34 min\n",
      "Epoch: 005/015 | Batch 000/098 | Cost: 0.0054\n",
      "Epoch: 005/015 | Batch 050/098 | Cost: 0.0049\n",
      "training accuracy: 100.00%\n",
      "Time elapsed: 5.56 min\n",
      "Epoch: 006/015 | Batch 000/098 | Cost: 0.0054\n",
      "Epoch: 006/015 | Batch 050/098 | Cost: 0.0038\n",
      "training accuracy: 100.00%\n",
      "Time elapsed: 6.76 min\n",
      "Epoch: 007/015 | Batch 000/098 | Cost: 0.0037\n",
      "Epoch: 007/015 | Batch 050/098 | Cost: 0.0030\n",
      "training accuracy: 100.00%\n",
      "Time elapsed: 7.78 min\n",
      "Epoch: 008/015 | Batch 000/098 | Cost: 0.0026\n",
      "Epoch: 008/015 | Batch 050/098 | Cost: 0.0024\n",
      "training accuracy: 100.00%\n",
      "Time elapsed: 8.77 min\n",
      "Epoch: 009/015 | Batch 000/098 | Cost: 0.0022\n",
      "Epoch: 009/015 | Batch 050/098 | Cost: 0.0021\n",
      "training accuracy: 100.00%\n",
      "Time elapsed: 34.52 min\n",
      "Epoch: 010/015 | Batch 000/098 | Cost: 0.0018\n",
      "Epoch: 010/015 | Batch 050/098 | Cost: 0.0018\n",
      "training accuracy: 100.00%\n",
      "Time elapsed: 35.42 min\n",
      "Epoch: 011/015 | Batch 000/098 | Cost: 0.0016\n",
      "Epoch: 011/015 | Batch 050/098 | Cost: 0.0018\n",
      "training accuracy: 100.00%\n",
      "Time elapsed: 36.30 min\n",
      "Epoch: 012/015 | Batch 000/098 | Cost: 0.0014\n",
      "Epoch: 012/015 | Batch 050/098 | Cost: 0.0013\n",
      "training accuracy: 100.00%\n",
      "Time elapsed: 37.19 min\n",
      "Epoch: 013/015 | Batch 000/098 | Cost: 0.0015\n",
      "Epoch: 013/015 | Batch 050/098 | Cost: 0.0013\n",
      "training accuracy: 100.00%\n",
      "Time elapsed: 38.05 min\n",
      "Epoch: 014/015 | Batch 000/098 | Cost: 0.0012\n",
      "Epoch: 014/015 | Batch 050/098 | Cost: 0.0011\n",
      "training accuracy: 100.00%\n",
      "Time elapsed: 39.04 min\n",
      "Epoch: 015/015 | Batch 000/098 | Cost: 0.0010\n",
      "Epoch: 015/015 | Batch 050/098 | Cost: 0.0010\n",
      "training accuracy: 100.00%\n",
      "Time elapsed: 39.63 min\n",
      "Total Training Time: 39.63 min\n",
      "Test accuracy: 50.00%\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch_idx, (label, text) in enumerate(train_loader):\n",
    "        \n",
    "        ### FORWARD AND BACK PROP\n",
    "        logits = model(text)\n",
    "        cost = F.binary_cross_entropy_with_logits(logits, label.float())\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        cost.backward()\n",
    "        \n",
    "        ### UPDATE MODEL PARAMETERS\n",
    "        optimizer.step()\n",
    "        \n",
    "        ### LOGGING\n",
    "        if not batch_idx % 50:\n",
    "            print (f'Epoch: {epoch+1:03d}/{num_epochs:03d} | '\n",
    "                   f'Batch {batch_idx:03d}/{len(list(train_loader)):03d} | '\n",
    "                   f'Cost: {cost:.4f}')\n",
    "\n",
    "    with torch.set_grad_enabled(False):\n",
    "        print(f'training accuracy: '\n",
    "              f'{compute_binary_accuracy(model, train_loader, device):.2f}%')\n",
    "              # f'\\nvalid accuracy: '\n",
    "              # f'{compute_binary_accuracy(model, valid_loader, device):.2f}%')\n",
    "        \n",
    "    print(f'Time elapsed: {(time.time() - start_time)/60:.2f} min')\n",
    "    \n",
    "print(f'Total Training Time: {(time.time() - start_time)/60:.2f} min')\n",
    "print(f'Test accuracy: {compute_binary_accuracy(model, test_loader, device):.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c308cfbc-958b-49a3-828d-adb7e00c880f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Predict sentiment from sentence\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def predict_sentiment(model, sentence):\n",
    "    # based on:\n",
    "    # https://github.com/bentrevett/pytorch-sentiment-analysis/blob/\n",
    "    # master/2%20-%20Upgraded%20Sentiment%20Analysis.ipynb\n",
    "    model.eval()\n",
    "    tokenized = [tok.text for tok in nlp.tokenizer(sentence)]\n",
    "    indexed = [vocab.get_stoi()[t] for t in tokenized]\n",
    "    tensor = torch.LongTensor(indexed).to(device)\n",
    "    tensor = tensor.unsqueeze(1)\n",
    "    prediction = torch.sigmoid(model(tensor))\n",
    "    return prediction.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "46c8459f-3226-4957-b73e-432cc5b1bf07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability positive:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9982149600982666"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Probability positive:')\n",
    "predict_sentiment(model, \"I really love this movie. This movie is so great!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9625bf-2e2e-4acd-b3a7-33152391bc9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "87b0e816-71b4-49d8-8935-f49675e7f90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Something wrong with the dataset\n",
    "## let's try using csv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
